#!/usr/bin/env python
import csv
import math
import sys
import time

import roslib; roslib.load_manifest('kitchen_data_player')
import rospy
import tf
from tf.transformations import quaternion_from_euler

from geometry_msgs.msg import Pose
from geometry_msgs.msg import PoseStamped
from geometry_msgs.msg import PoseArray

# This script parses poses.csv and labels.csv of the TUM kitchen dataset and creates a new csv-file that contains a symbolic
# version of the humans table-setting plan including durations 

# Usage: stationary_kitchen_data.py poses.csv labels.csv symbolic_plan.csv

# This function checks in which cluster the given point is and assigns the correct location to it. 
# The multivariate gaussians are hardcoded here and generated by WEKA from the TUM kitchen dataset
def get_cluster_number(x, y):
    
    x = x * 1000
    y = y * 1000

    # Those are the hardcoded gaussians from WEKA
    p1 = math.exp( - ( (((x - 971.8) * (x - 971.8)) / (2 * 77.2 * 77.2)) + ((y - 2619.5) * (y - 2619.5)) / (2 * 98.55 * 98.55)))
    p2 = math.exp( - ( (((x - 2174.9577) * (x - 2174.9577)) / (2 * 91.9 * 91.9)) + ((y - 2643.1) * (y - 2643.1)) / (2 * 107.5 * 107.5)))
    p3 = math.exp( - ( (((x - 763.71) * (x - 763.71)) / (2 * 61.9 * 61.9)) + ((y - 2173.98) * (y - 2173.98)) / (2 * 136.96 * 136.96))) 
    p4 = math.exp( - ( (((x -778.4352) * (x - 778.4352)) / (2 * 70.6725 * 70.6725)) + ((y - 3215.1622) * (y - 3215.1622)) / (2 * 72.6388 * 72.6388)))

    cluster = -1
    location = ''
    threshhold = 0.0001

    if p1 > p2 and p1 > p3 and p1 > p4 and p1 > threshhold:
        cluster = 1
        location = 'drawer'
        #print("Human is in cluster %s with prob: %s"%(location, p1))
    elif p2 > p1 and p2 > p3 and p2 > p4 and p2 > threshhold:
        cluster = 2
        location = 'table'
        #print("Human is in cluster %s with prob: %s"%(location, p2))
    elif p3 > p1 and p3 > p2 and p3 > p4 and p3 > threshhold:
        cluster = 3
        location = 'stove'
        #print("Human is in cluster %s with prob: %s"%(location, p3))
    elif p4 > p1 and p4 > p2 and p4 > p3 and p4 > threshhold:
        cluster = 4
        location = 'cupboard'
        #print("Human is in cluster %s with prob: %s"%(location, p4))
    else:
        cluster = -1
        location = 'none'
        #print("WARNING, could not find location of that object (p1,p2,p3,p4): %s, %s, %s, %s"%(p1, p2, p3, p4))
    p1 = 0
    p2 = 0
    p3 = 0
    p4 = 0
    return cluster, location

# publishes the mean-values of the gaussians for the human positions generated by WEKA
def publish_gaussians():
    # Use TF to calculate relative area definitions
    br.sendTransform((0.9718, 2.6195, 0),
                     quaternion_from_euler(0,0,3.1415) ,
                     rospy.Time.now(),
                     "drawer_gaussian_mean",
                     "map")

    br.sendTransform((2.17495, 2.6431, 0),
                     quaternion_from_euler(0,0,0) ,
                     rospy.Time.now(),
                     "table_gaussian_mean",
                     "map")
    
    br.sendTransform((0.76371, 2.17398, 0),
                     quaternion_from_euler(0,0,3.1415) ,
                     rospy.Time.now(),
                     "stove_gaussian_mean",
                     "map")

    br.sendTransform((0.77843, 3.21516, 0),
                     quaternion_from_euler(0,0,3.1415) ,
                     rospy.Time.now(),
                     "cupboard_gaussian_mean",
                     "map")


# Publishes the (hard-coded) locations of the furniture using tf
def publish_furniture():

    #br = tf.TransformBroadcaster()
    # Furniture

    # from logged object data
    plate_x = 2.60843
    plate_y = 2.62942
    plate_theta = 3.1415
    
    # from semantic map
    table_x = 2.78
    table_y = 2.29
    table_theta = -1.5707963705062866
    table_depth = 0.8
    table_theta = 1.57075

    # from logged object data
    placemat_x = 0.3815425
    placemat_y = 2.0313084
    placemat_theta = 0
    
    # from sematic map
    stove_x = 0.32918193
    stove_y = 1.9835850
    stove_theta = 0
    stove_depth = 0.5766061

    # from semantic map
    drawer_x = 0.32296494
    drawer_y = 2.530835
    drawer_theta = 0
    drawer_depth = 0.5641721

    # from semantic map
    cupboard_x = 0.19635946
    cupboard_y = 3.107985
    cupboard_theta = 0
    cupboard_depth = 0.3109611

    br.sendTransform((table_x, table_y, 0),
                     tf.transformations.quaternion_from_euler(0, 0, table_theta),
                     rospy.Time.now(),
                     "table",
                     "map")
                     
    br.sendTransform((0, table_depth/2, 0),
                     tf.transformations.quaternion_from_euler(0, 0, 0),
                     rospy.Time.now(),
                     "table_edge",
                     "table")
    
    br.sendTransform((plate_x, plate_y, 0),
                     tf.transformations.quaternion_from_euler(0, 0, plate_theta),
                     rospy.Time.now(),
                     "plate",
                     "map")

    br.sendTransform((drawer_x, drawer_y, 0),
                     tf.transformations.quaternion_from_euler(0, 0, drawer_theta),
                     rospy.Time.now(),
                     "drawer",
                     "map")
    
    # Edge of drawer as reference to where human is standing                 
    br.sendTransform((drawer_depth/2, 0, 0),
                     tf.transformations.quaternion_from_euler(0, 0, 0),
                     rospy.Time.now(),
                     "drawer_edge",
                     "drawer")

    br.sendTransform((stove_x, stove_y, 0),
                     tf.transformations.quaternion_from_euler(0, 0, stove_theta),
                     rospy.Time.now(),
                     "stove",
                     "map")
                     
    # object that is on the stove
    br.sendTransform((placemat_x, placemat_y, 0),
                     tf.transformations.quaternion_from_euler(0, 0, placemat_theta),
                     rospy.Time.now(),
                     "placemat",
                     "map")
    
    br.sendTransform((stove_depth/2, 0, 0),
                     tf.transformations.quaternion_from_euler(0, 0, stove_theta),
                     rospy.Time.now(),
                     "stove_edge",
                     "stove")

    br.sendTransform((cupboard_x, cupboard_y, 0),
                     tf.transformations.quaternion_from_euler(0, 0, cupboard_theta),
                     rospy.Time.now(),
                     "cupboard",
                     "map")
                    
    br.sendTransform((cupboard_depth/2, 0, 0),
                     tf.transformations.quaternion_from_euler(0, 0, 0),
                     rospy.Time.now(),
                     "cupboard_edge",
                     "cupboard")
    try:
        now = rospy.Time.now() - rospy.Duration(0.0)
        tf_listener.waitForTransform("stove_edge", "placemat", rospy.Time(0), rospy.Duration(0.001))
        (placemat_trans, placemat_rot) = tf_listener.lookupTransform("stove_edge", "placemat", rospy.Time(0))
        tf_listener.waitForTransform("table_edge", "plate", rospy.Time(0), rospy.Duration(0.001))
        (plate_trans, plate_rot) = tf_listener.lookupTransform("table_edge", "plate", rospy.Time(0))
        #print("Trans: %s, %s, %s"%(trans[0], trans[1], trans[2]))
        
        br.sendTransform((0, placemat_trans[1], 0),
                     tf.transformations.quaternion_from_euler(0, 0, 0),
                     rospy.Time.now(),
                     "placemat_edge",
                     "stove_edge")
                     
        br.sendTransform((plate_trans[0], 0, 0),
                     tf.transformations.quaternion_from_euler(0, 0, plate_theta - table_theta),
                     rospy.Time.now(),
                     "plate_edge",
                     "table_edge")
    except (tf.Exception, tf.LookupException, tf.ConnectivityException):
        print('.')

            
posesReader = csv.DictReader(open(sys.argv[1], 'rb'), delimiter=',', quotechar='|')
labelsReader = csv.DictReader(open(sys.argv[2], 'rb'), delimiter=',', quotechar='|')

# Init ROSnode
pose_pub = rospy.Publisher('kitchen_pose', PoseStamped)             # current pose
poses_pub = rospy.Publisher('kitchen_poses', PoseArray)             # all poses
obj_pub = rospy.Publisher('object_poses', PoseArray)                # object positions

rospy.init_node('kitchen_player')

human_positions = PoseArray()                     # array to store the poses of the human for display
obj_positions = PoseArray()                       # array to store object positions for display
human_positions.header.frame_id = 'map'
obj_positions.header.frame_id = 'map'
last_lefthand = 'start'
last_righthand = 'start'
br = tf.TransformBroadcaster()
tf_listener = tf.TransformListener()

# Init cluster and location
cluster = None
location = None
last_location = None
final_location = None
final_time = 0
timesteps = 0
true_location = None
true_time = 0
semantic_instance = 0
FILE = open(sys.argv[3],"w")
FILE.write("instance,location,duration\n") 


for row in posesReader:
    publish_furniture()
    publish_gaussians()
    #time.sleep(1/2)
    # Write new csv-file with: instance, time, BECX, BEXY, BECTheta, 
    theta = 0
    # Calculate theta
    instance = row['instance']
    x = float(row['SBRX']) - float(row['SBLX'])
    y = float(row['SBRY']) - float(row['SBLY'])
    
    #theta = math.atan2(-y, x)
    theta = math.atan2(x, -y)
    trunk = 'NONE'
    lefthand = 'NONE'
    righthand = 'NONE'


    # The following code ist only used for visualization for now
    quat = quaternion_from_euler(0,0,theta)
    br.sendTransform((float(row['BECX'])/1000, float(row['BECY'])/1000, 0),
                     quat,
                     rospy.Time.now(),
                     "human_pose",
                     "map")

    for labelrow in labelsReader:
        labelsReader = csv.DictReader(open(sys.argv[2], 'rb'), delimiter=',', quotechar='|')
        if instance == labelrow['instance']:
            lefthand = labelrow['lefthand']
            righthand = labelrow['righthand']
            trunk = labelrow['trunk']
             
    # Calculate Human position when standing still and interacting with objects
    if trunk == 'StandingStill'\
                 and (((lefthand == 'TakingSomeThing' and last_lefthand != 'TakingSomeThing') \
                 or (lefthand == 'ReleasingGraspOfSomething' and last_lefthand != 'ReleasingGraspOfSomething' and last_lefthand != 'ClosingADoor')) \
                 or ((righthand == 'TakingSomething' and last_righthand !=  'TakingSomething') \
                 or (righthand == 'ReleasingGraspOfSomething' and last_righthand !='ReleasingGraspOfSomething' and last_righthand != 'ClosingADoor'))):
        
        # Publish information on ROS-topics on the fly
        pose = PoseStamped()
        pose.header.frame_id = 'map'
        pose.pose.position.x = float(row['BECX'])/1000 # convert to meters here
        pose.pose.position.y = float(row['BECY'])/1000 # convert to meters here
        quat = quaternion_from_euler(0,0,theta)
        pose.pose.orientation.x = float(quat[0])
        pose.pose.orientation.y = float(quat[1])
        pose.pose.orientation.z = float(quat[2])
        pose.pose.orientation.w = float(quat[3])

        human_positions.poses.append(pose.pose)
        
        #print("TF from table: %s, %s"%tf_listener.lookupTransform("human_pose", "table", rospy.Time(0)))
        poses_pub.publish(human_positions)
        pose_pub.publish(pose)

    # Calculate object positions when grasping or releasing objects and calculate orientation of object to human (obj-pos looking at human body center)
    if trunk == 'StandingStill': 
        left = False
        right = False
        left_pose = Pose()
        right_pose = Pose()
        left_theta = 0
        right_theta = 0
        
        if ((lefthand == 'TakingSomething' and last_lefthand !=  'TakingSomething') \
                or (lefthand == 'ReleasingGraspOfSomething' and last_lefthand != 'ReleasingGraspOfSomething' and last_lefthand != 'ClosingADoor')): # Object interation with left hand
            left = True
            left_pose = Pose()
            left_pose.position.x = float(row['HALX'])/1000
            left_pose.position.y = float(row['HALY'])/1000

            # calculate object orientation using object position and human position (without orientation)
            # obj_quat = quaternion_from_euler(0,0,theta + 3.1415)
            deltax = pose.pose.position.x - left_pose.position.x
            deltay = pose.pose.position.y - left_pose.position.y
            left_theta = math.atan2(deltay, deltax)
            obj_quat = quaternion_from_euler(0,0,left_theta)
            
            left_pose.orientation.x = float(obj_quat[0])
            left_pose.orientation.y = float(obj_quat[1])
            left_pose.orientation.z = float(obj_quat[2])
            left_pose.orientation.w = float(obj_quat[3])
            #obj_positions.poses.append(obj_pose)
            
            cluster, location = get_cluster_number(float(row['BECX'])/1000, float(row['BECY'])/1000)

        if ((righthand == 'TakingSomething' and last_righthand !=  'TakingSomething') \
                or (righthand == 'ReleasingGraspOfSomething' and last_righthand !='ReleasingGraspOfSomething' and last_righthand != 'ClosingADoor')): # Object interaction with right hand
            #obj_pose = Pose()
            right = True
            right_pose.position.x = float(row['HARX'])/1000 
            right_pose.position.y = float(row['HARY'])/1000

            # calculate object orientation using object position and human position (without orientation)
            # obj_quat = quaternion_from_euler(0,0,theta + 3.1415)
            deltax = pose.pose.position.x - right_pose.position.x
            deltay = pose.pose.position.y - right_pose.position.y
            right_theta = math.atan2(deltay,deltax)
            obj_quat = quaternion_from_euler(0,0,right_theta)

            right_pose.orientation.x = float(obj_quat[0])
            right_pose.orientation.y = float(obj_quat[1])
            right_pose.orientation.z = float(obj_quat[2])
            right_pose.orientation.w = float(obj_quat[3])
            #obj_positions.poses.append(obj_pose)
            
            cluster, location = get_cluster_number(float(row['BECX'])/1000, float(row['BECY'])/1000)
        
        if right == True and left == False:            # Object interactions with right hand
            obj_positions.poses.append(right_pose)
        if left == True and right == False:            # Object interactions with left hand
            obj_positions.poses.append(left_pose)
        if left == True and right == True:             # Object interactions with both hands => use average point between 2 hand as reference and 
            #obj_positions.poses.append(left_pose)      # calculate orientation to human from there
            #obj_positions.poses.append(right_pose)
            avg_pose = Pose()
            avg_pose.position.x = float((left_pose.position.x + right_pose.position.x) / 2)
            avg_pose.position.y = float((left_pose.position.y + right_pose.position.y) / 2)
            
            deltax = pose.pose.position.x - avg_pose.position.x
            deltay = pose.pose.position.y - avg_pose.position.y
            avg_theta = math.atan2(deltay,deltax)
            avg_quat = quaternion_from_euler(0,0,avg_theta)
            
            avg_pose.orientation.x = float(avg_quat[0])
            avg_pose.orientation.y = float(avg_quat[1])
            avg_pose.orientation.z = float(avg_quat[2])
            avg_pose.orientation.w = float(avg_quat[3])
            obj_positions.poses.append(avg_pose)
    
        obj_pub.publish(obj_positions)
        last_lefthand = lefthand
        last_righthand = righthand
        
    # ************ MODEL BUILDING START
    # Here the actual model building starts. First get cluster and location of current human pose
    cluster, location = get_cluster_number(float(row['BECX'])/1000,float(row['BECY'])/1000)
    
    if location == last_location:
        timesteps += 1
    if location != last_location:
        # WARNING: This causes problems especially due to the stove and drawer positions beeing so close to 
        # each other.Maybe here we should generate hypotheses, that are validated using the object positions 
        # that the human acts on, too... or for the moment just check in which cluster the human stays the longest
        # and do not account for the time in between
        
        final_location = last_location #only stored for the final location that is NOT navigation
        final_time = float(timesteps)/30

        if last_location != "none":
            time = float(timesteps)/30 # motion tracking data recorded at 30 Hz
            if time > true_time:       # Here use the location candidate with the longest duration
                true_time = time
                true_location = last_location
        else:                          # When human is navigation again, choose the location candidate with the highest duration and output it
            print("%s,%s,%s"%(semantic_instance, true_location, true_time))    
            FILE.write("%s,%s,%s\n"%(semantic_instance, true_location, true_time))
            semantic_instance += 1
            FILE.write("%s,navigation,%s\n"%(semantic_instance, float(timesteps)/30)) # Output navigation duration
            print("%s,navigation,%s"%(semantic_instance, float(timesteps)/30))
            semantic_instance += 1
            true_time = 0
        timesteps = 0

    last_location = location
    # ************ MODEL BUILDING END

# last object action is not covered by if cases above, so output it when all human poses have been precessed
# Two cases have to be considered here: Either the human is within a valid location (most likely table) when
# Motion capturing stops or it is already outside a valid location
if final_location == 'none':
    print("%s,%s,%s"%(semantic_instance, last_location, float(timesteps)/30)) 
    FILE.write("%s,%s,%s\n"%(semantic_instance, last_location, float(timesteps)/30))
else:
    print("%s,%s,%s"%(semantic_instance, final_location, final_time)) 
    FILE.write("%s,%s,%s\n"%(semantic_instance, final_location, final_time))

semantic_instance += 1
FILE.write("%s,navigation,%s\n"%(semantic_instance, 0))
print("%s,navigation,%s"%(semantic_instance, 0))
