#!/usr/bin/env python
import csv
import math
import sys
import time

import roslib; roslib.load_manifest('kitchen_data_player')
import rospy
import tf
from tf.transformations import quaternion_from_euler

from geometry_msgs.msg import Pose
from geometry_msgs.msg import PoseStamped
from geometry_msgs.msg import PoseArray

# This script parses poses.csv and labels.csv of the TUM kitchen dataset and creates a new csv-file that contains positions, labels
# and timestamps of poses where the human stood still and interacted with objects
# Here we only consider poses where the labels change in a way that "ReleaseAnObject" must be preceeded by NOT "ReleaseAnObject" and so on
# Also Object positions are calculated for those positions and their orientation is calculated from the pose of the humans hand at the
# specific time towards the human position

# Usage: stationary_kitchen_data.py poses.csv labels.csv output.csv

# This function checks in which cluster the given point is and assigns the correct location to it. The multivariate gaussians are hardcoded here and generated by WEKA
def get_cluster_number(x, y):
    
    x = x * 1000
    y = y * 1000

    # Those are the hardcoded gaussians from WEKA
    p1 = math.exp( - ( (((x - 971.8) * (x - 971.8)) / (2 * 77.2 * 77.2)) + ((y - 2619.5) * (y - 2619.5)) / (2 * 98.55 * 98.55)))
    p2 = math.exp( - ( (((x - 2174.9577) * (x - 2174.9577)) / (2 * 91.9 * 91.9)) + ((y - 2643.1) * (y - 2643.1)) / (2 * 107.5 * 107.5)))
    p3 = math.exp( - ( (((x - 763.71) * (x - 763.71)) / (2 * 61.9 * 61.9)) + ((y - 2173.98) * (y - 2173.98)) / (2 * 136.96 * 136.96))) 
    p4 = math.exp( - ( (((x -778.4352) * (x - 778.4352)) / (2 * 70.6725 * 70.6725)) + ((y - 3215.1622) * (y - 3215.1622)) / (2 * 72.6388 * 72.6388)))

    cluster = -1
    location = ''

    if p1 > p2 and p1 > p3 and p1 > p4:
        cluster = 1
        location = 'drawer'
    if p2 > p1 and p2 > p3 and p2 > p4:
        cluster = 2
        location = 'table'
    if p3 > p1 and p3 > p2 and p3 > p4:
        cluster = 3
        location = 'stove'
    if p4 > p1 and p4 > p2 and p4 > p3:
        cluster = 4
        location = 'cupboard'

    return cluster, location

posesReader = csv.DictReader(open(sys.argv[1], 'rb'), delimiter=',', quotechar='|')
labelsReader = csv.DictReader(open(sys.argv[2], 'rb'), delimiter=',', quotechar='|')

FILE = open(sys.argv[3],"w")
FILE.write("instance,time,OBJX,OBJY,OBJTHETA,cluster,location\n") 

# Init ROSnode
pose_pub = rospy.Publisher('kitchen_pose', PoseStamped)             # current pose
poses_pub = rospy.Publisher('kitchen_poses', PoseArray)             # all poses
obj_pub = rospy.Publisher('object_poses', PoseArray)                # object positions
rospy.init_node('kitchen_player')

human_positions = PoseArray()                     # array to store the poses of the human for display
obj_positions = PoseArray()                       # array to store object positions for display
human_positions.header.frame_id = 'map'
obj_positions.header.frame_id = 'map'
last_lefthand = 'start'
last_righthand = 'start'


for row in posesReader:
    # Write new csv-file with: instance, time, BECX, BEXY, BECTheta, 
    theta = 0
    # Calculate theta
    instance = row['instance']
    x = float(row['SBRX']) - float(row['SBLX'])
    y = float(row['SBRY']) - float(row['SBLY'])
    #theta = math.atan2(-y, x)
    theta = math.atan2(x, -y)
    trunk = 'NONE'
    lefthand = 'NONE'
    righthand = 'NONE'
    for labelrow in labelsReader:
        labelsReader = csv.DictReader(open(sys.argv[2], 'rb'), delimiter=',', quotechar='|')
        if instance == labelrow['instance']:
            lefthand = labelrow['lefthand']
            righthand = labelrow['righthand']
            trunk = labelrow['trunk']
             
    # Calculate Human position when standing still and interacting with objects
    if trunk == 'StandingStill' and (((lefthand == 'TakingSomeThing' and last_lefthand != 'TakingSomeThing') or (lefthand == 'ReleasingGraspOfSomething' and last_lefthand != 'ReleasingGraspOfSomething')) or ((righthand == 'TakingSomething' and last_righthand !=  'TakingSomething') or (righthand == 'ReleasingGraspOfSomething' and last_righthand !='ReleasingGraspOfSomething'))):
        
        # Publish information on ROS-topics on the fly
        pose = PoseStamped()
        pose.header.frame_id = 'map'
        pose.pose.position.x = float(row['BECX'])/1000 # convert to meters here
        pose.pose.position.y = float(row['BECY'])/1000 # convert to meters here
        quat = quaternion_from_euler(0,0,theta)
        pose.pose.orientation.x = float(quat[0])
        pose.pose.orientation.y = float(quat[1])
        pose.pose.orientation.z = float(quat[2])
        pose.pose.orientation.w = float(quat[3])

        human_positions.poses.append(pose.pose)
        
        poses_pub.publish(human_positions)
        pose_pub.publish(pose)
        # time.sleep(1/30) # Add this if you want to play in realtime

    # Calculate object positions when grasping or releasing objects and calculate orientation of object to human (obj-pos looking at human body center)
    if trunk == 'StandingStill': 
        obj_pose = Pose()
        if ((lefthand == 'TakingSomeThing' and last_lefthand != 'TakingSomeThing') or (lefthand == 'ReleasingGraspOfSomething' and last_lefthand != 'ReleasingGraspOfSomething')): # Object interation with left hand
            obj_pose.position.x = float(row['HALX'])/1000
            obj_pose.position.y = float(row['HALY'])/1000

            # calculate object orientation using object position and human position (without orientation)
            # obj_quat = quaternion_from_euler(0,0,theta + 3.1415)
            deltax = pose.pose.position.x - obj_pose.position.x
            deltay = pose.pose.position.y - obj_pose.position.y
            obj_theta = math.atan2(deltay, deltax)
            obj_quat = quaternion_from_euler(0,0,obj_theta)
            
            obj_pose.orientation.x = float(obj_quat[0])
            obj_pose.orientation.y = float(obj_quat[1])
            obj_pose.orientation.z = float(obj_quat[2])
            obj_pose.orientation.w = float(obj_quat[3])

            cluster, location = get_cluster_number(pose.pose.position.x, pose.pose.position.y)

            FILE.write("%s,%s,%s,%s,%s,%s,%s\n"%(instance, row['time'], row['HALX'], row['HALY'], obj_theta, cluster, location))
            print("Object position: %s,%s,%s,%s,%s,%s,%s"%(instance, row['time'], row['HALX'], row['HALY'], obj_theta, cluster, location))

        if ((righthand == 'TakingSomething' and last_righthand !=  'TakingSomething') \
                or (righthand == 'ReleasingGraspOfSomething' and last_righthand !='ReleasingGraspOfSomething')): # Object interaction with right hand
            obj_pose.position.x = float(row['HARX'])/1000 
            obj_pose.position.y = float(row['HARY'])/1000

            # calculate object orientation using object position and human position (without orientation)
            # obj_quat = quaternion_from_euler(0,0,theta + 3.1415)
            deltax = pose.pose.position.x - obj_pose.position.x
            deltay = pose.pose.position.y - obj_pose.position.y
            obj_theta = math.atan2(deltay,deltax)
            obj_quat = quaternion_from_euler(0,0,obj_theta)

            obj_pose.orientation.x = float(obj_quat[0])
            obj_pose.orientation.y = float(obj_quat[1])
            obj_pose.orientation.z = float(obj_quat[2])
            obj_pose.orientation.w = float(obj_quat[3])
            
            cluster, location = get_cluster_number(pose.pose.position.x, pose.pose.position.y)

            FILE.write("%s,%s,%s,%s,%s,%s,%s\n"%(instance, row['time'], row['HALX'], row['HALY'], obj_theta, cluster, location))
            print("Object position: %s,%s,%s,%s,%s,%s,%s"%(instance, row['time'], row['HALX'], row['HALY'], obj_theta, cluster, location))

        obj_positions.poses.append(obj_pose)
        obj_pub.publish(obj_positions)
        last_lefthand = lefthand
        last_righthand = righthand
        

